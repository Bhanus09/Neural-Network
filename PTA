import numpy as np
import matplotlib.pylab as plt

%matplotlib inline

w0 = np.random.uniform(-0.25, 0.25)         # Bias
w1 = np.random.uniform(-1, 1)
w2 = np.random.uniform(-1, 1)
W_Original = [w0, w1, w2]
print ('The original weights: ', W_Original)

S = np.random.uniform(-1, 1, size=(100, 2)) 
S0 = []
S1 = []
for i in S:
    if (1*w0)+(i[0]*w1)+(i[1]*w2) >= 0:
            S1.append([i[0]] + [i[1]] + [0])
    elif (i[0]*w1)+(i[1]*w2) < 0:
            S0.append([i[0]] + [i[1]] + [1])

dataset = S0 + S1                      # Complete dataset
print(dataset)


x1 = -(w0-w2)/w1
x2 = -(w0+w2)/w1
X = np.array([x1, x2])
Y = np.array([-1.0, +1.0])

S1_x = []
S1_y = []
S0_x = []
S0_y = []

for i in S0:
    S0_x.append(i[0])
    S0_y.append(i[1])
for i in S1:
    S1_x.append(i[0])
    S1_y.append(i[1])

fig, ax = plt.subplots(figsize=(10,10))
blue = plt.scatter(S0_x, S0_y, c ='b', marker ='+', label='S1 - Class 1')
red = plt.scatter(S1_x, S1_y, c='r',marker ='^', label='S0 - Class 0')
line = ax.plot(X, Y, c = 'green', label='Boundry')
plt.title('Question 3(i)')
plt.legend(loc="upper right")
plt.ylim([-1,1])
plt.xlim([-1,1])
plt.show()

# Unit Step Activation Function  
def activation_fn(x):
    if x >= 0:
        y = 1
    else:
        y = 0
    return y
w0_1 = np.random.uniform(-1, 1)
w1_1 = np.random.uniform(-1, 1)
w2_1 = np.random.uniform(-1, 1)

W = []
W = [w0_1, w1_1, w2_1]

def misclassified(dataset, W):
    misclassifications = 0
    for each in dataset:
        y = (W[0]+(each[0]*W[1])+(each[1]*W[2]))
        y = activation_fn(y)
        if y != each[2]:
            misclassifications = misclassifications +1
    return misclassifications
a = misclassified(dataset, W)
print ('Number of misclassifications: ', a)

def perceptron_training(omega):
    epoch = 0
    omegas = []
    missed = []
    while (misclassified(dataset,omega)!=0):
        missed.append(misclassified(dataset,omega))
        print ('Number of missclassifications: ', missed[epoch])
        epoch = epoch + 1
        print ('Epoch Number: ', epoch)
        for each in range(len(dataset)):
            y = omega[0] + (dataset[each][0]*omega[1]) + (dataset[each][1]*omega[2])
            y = activation_fn(y)
            updated_input =[1]+dataset[each][0:2]
            desired_output = dataset[each][2]
            difference = desired_output-y
            if difference != 0:
                updated_input[0]= updated_input[0]*learning_rate*difference
                updated_input[1]= updated_input[1]*learning_rate*difference
                updated_input[2]= updated_input[2]*learning_rate*difference
                omega[0] = omega[0]+updated_input[0]
                omega[1] = omega[1]+updated_input[1]
                omega[2] = omega[2]+updated_input[2]
        print ('Updated weights: ', omega)
        omegas.append(omega)
    final_misclassification = misclassified(dataset,omega)
    print ('Number of missclassifications: ', final_misclassification)
    print ('Final weights: ', omegas[-1])
    return omegas, missed

learning_rate = 1
print ('Initial weight: ' , W)
omegas=[]
omegas, missed = perceptron_training(W)
n_epochs = range(len(omegas)+1)
fig, ax = plt.subplots(figsize=(10,10))
ax.plot(n_epochs, missed+[0], c = 'green')
plt.ylabel('Number of Misclassifications')

omega = [w0_1, w1_1, w2_1]
learning_rate = 10
print ('Initial weight: ' , omega)
omegas=[]
omegas, missed = perceptron_training(omega)
n_epochs = range(len(omegas)+1)
fig, ax = plt.subplots(figsize=(10,10))
ax.plot(n_epochs, missed+[0], c = 'green')
plt.ylabel('Number of Misclassifications')
plt.xlabel('Number of Epochs')
plt.show()

omega = [w0_1, w1_1, w2_1]
learning_rate = 0.1       
print ('Initial weight: ' , omega)
omegas=[]
omegas, missed = perceptron_training(omega)
n_epochs = range(len(omegas)+1)
fig, ax = plt.subplots(figsize=(10,10))
ax.plot(n_epochs, missed+[0], c = 'green')
plt.ylabel('Number of Misclassifications')
plt.xlabel('Number of Epochs')
plt.show()
